{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a493ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from torch import nn, optim\n",
    "from transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig, \\\n",
    "    get_linear_schedule_with_warmup, XLNetModel, XLNetTokenizer, XLNetConfig\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score, roc_auc_score\n",
    "from model import *\n",
    "from utils import *\n",
    "import time\n",
    "import gc\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"train.log\",filemode='a')\n",
    "\n",
    "\n",
    "from NEZHA.modeling_nezha import *\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'BertForClass': BertForClass,\n",
    "    'BertLastCls': BertLastCls,\n",
    "    'BertLastTwoCls': BertLastTwoCls,\n",
    "    'BertLastTwoClsPooler': BertLastTwoClsPooler,\n",
    "    'BertLastTwoEmbeddings': BertLastTwoEmbeddings,\n",
    "    'BertLastTwoEmbeddingsPooler': BertLastTwoEmbeddingsPooler,\n",
    "    'BertLastFourCls': BertLastFourCls,\n",
    "    'BertLastFourClsPooler': BertLastFourClsPooler,\n",
    "    'BertLastFourEmbeddings': BertLastFourEmbeddings,\n",
    "    'BertLastFourEmbeddingsPooler': BertLastFourEmbeddingsPooler,\n",
    "    'BertDynCls': BertDynCls,\n",
    "    'BertDynEmbeddings': BertDynEmbeddings,\n",
    "    'BertRNN': BertRNN,\n",
    "    'BertCNN': BertCNN,\n",
    "    'BertRCNN': BertRCNN,\n",
    "    'XLNet': XLNet,\n",
    "    'Electra': Electra,\n",
    "    'NEZHA': NEZHA,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # 预训练模型路径\n",
    "        self.modelId = 2\n",
    "        self.model = \"NEZHA\"\n",
    "        self.Stratification = False\n",
    "        self.model_path = '../pretrain/nezha_model/'\n",
    "\n",
    "        self.num_class = 4\n",
    "        self.dropout = 0.2\n",
    "        self.MAX_LEN = 64\n",
    "        self.epoch = 3\n",
    "        self.learn_rate = 4e-5\n",
    "        self.normal_lr = 1e-4\n",
    "        self.batch_size = 32\n",
    "        self.k_fold = 5\n",
    "        self.seed = 42\n",
    "\n",
    "        self.device = torch.device('cuda')\n",
    "        # self.device = torch.device('cpu')\n",
    "\n",
    "        self.focalloss = False\n",
    "        self.pgd = False\n",
    "        self.fgm = True\n",
    "\n",
    "\n",
    "config = Config()\n",
    "os.environ['PYTHONHASHSEED']='0'#消除hash算法的随机性\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed_all(config.seed)\n",
    "\n",
    "\n",
    "file_path = './log/'\n",
    "# 创建一个logger\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cb3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_df = pd.read_csv('../../data/preliminary_sel_log_dataset.csv')\n",
    "test_log_df = pd.read_csv('../../data/preliminary_sel_log_dataset_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24c72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = pd.read_csv('../../data/preliminary_train_label_dataset.csv')\n",
    "label2 = pd.read_csv('../../data/preliminary_train_label_dataset_s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591db2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.concat([label1, label2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fec2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_csv('../../log_template.csv')\n",
    "log_df['time'] = pd.to_datetime(log_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1322c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['fault_time'] = pd.to_datetime(label_df['fault_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64276cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3627910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16669/16669 [08:27<00:00, 32.83it/s]\n"
     ]
    }
   ],
   "source": [
    "label_df['start_fault_time'] = label_df['fault_time'] - pd.Timedelta(pd.offsets.Hour(3))\n",
    "\n",
    "tr_data = []\n",
    "for i in tqdm(range(label_df.shape[0])):\n",
    "    row = label_df.iloc[i]\n",
    "    sn = row['sn']\n",
    "    label = row['label']\n",
    "    start_fault_time = row['start_fault_time']\n",
    "    fault_time = row['fault_time']\n",
    "    tmp = log_df[log_df['sn'] == sn]\n",
    "    tmp = tmp[(tmp['time'] >= start_fault_time) & (tmp['time'] <= fault_time)].sort_values(by='time', ascending=False)\n",
    "    text = ' '.join(tmp['template_id'].values.astype(str).tolist())\n",
    "    tr_data.append([text, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c782f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(tr_data)\n",
    "train.columns=['text', 'label']\n",
    "\n",
    "train_text = train['text'].values.astype(str)\n",
    "train_label = train['label'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86256ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = np.zeros((len(train), config.num_class), dtype=np.float32)\n",
    "\n",
    "kf = KFold(n_splits=config.k_fold, shuffle=True, random_state=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f8af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param target_df: [sn,fault_time,label]\n",
    "    :param submit_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    weights =  [3  /  7,  2  /  7,  1  /  7,  1  /  7]\n",
    "    overall_df = pd.DataFrame([y_true, y_pred]).T\n",
    "    overall_df.columns = ['label_gt', 'label_pr']\n",
    "\n",
    "    macro_F1 =  0.\n",
    "    for i in  range(len(weights)):\n",
    "        TP =  len(overall_df[(overall_df['label_gt'] == i) & (overall_df['label_pr'] == i)])\n",
    "        FP =  len(overall_df[(overall_df['label_gt'] != i) & (overall_df['label_pr'] == i)])\n",
    "        FN =  len(overall_df[(overall_df['label_gt'] == i) & (overall_df['label_pr'] != i)])\n",
    "        precision = TP /  (TP + FP)  if  (TP + FP)  >  0  else  0\n",
    "        recall = TP /  (TP + FN)  if  (TP + FP)  >  0  else  0\n",
    "        F1 =  2  * precision * recall /  (precision + recall)  if  (precision + recall)  >  0  else  0\n",
    "        macro_F1 += weights[i]  * F1\n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f249c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/projects/log-based-failuer-diagnosis/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n",
      "You are using a model of type bert to instantiate a model of type nezha. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------fold:0------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrain/nezha_model/ were not used when initializing NeZhaModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ../pretrain/nezha_model/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------epoch:0------------\n",
      "微调第0轮耗时：479.86598658561707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/projects/log-based-failuer-diagnosis/env/lib/python3.7/site-packages/ipykernel_launcher.py:127: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_f1:0.4490171315711199 \n",
      "\n",
      "\n",
      "------------epoch:1------------\n",
      "微调第1轮耗时：477.14072132110596\n",
      "best_f1:0.4538933037248219 \n",
      "\n",
      "\n",
      "------------epoch:2------------\n",
      "微调第2轮耗时：481.94307470321655\n",
      "best_f1:0.451569933952419 \n",
      "\n",
      "\n",
      "\n",
      "------------fold:1------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type nezha. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ../pretrain/nezha_model/ were not used when initializing NeZhaModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ../pretrain/nezha_model/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------epoch:0------------\n",
      "微调第0轮耗时：480.3106963634491\n",
      "best_f1:0.44686018122433036 \n",
      "\n",
      "\n",
      "------------epoch:1------------\n",
      "微调第1轮耗时：481.5736083984375\n",
      "best_f1:0.4559522565253009 \n",
      "\n",
      "\n",
      "------------epoch:2------------\n",
      "微调第2轮耗时：481.2147641181946\n",
      "best_f1:0.45656997227947166 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type nezha. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------fold:2------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrain/nezha_model/ were not used when initializing NeZhaModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ../pretrain/nezha_model/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------epoch:0------------\n",
      "微调第0轮耗时：482.060133934021\n",
      "best_f1:0.4388272663935034 \n",
      "\n",
      "\n",
      "------------epoch:1------------\n",
      "微调第1轮耗时：483.7433087825775\n",
      "best_f1:0.4556022332733586 \n",
      "\n",
      "\n",
      "------------epoch:2------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10647/986054403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfgm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# 对抗训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mfgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 在embedding上添加对抗扰动\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/projects/log-based-failuer-diagnosis/nezha-base-count3/finetuning/utils.py\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, epsilon, emb_name)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                     \u001b[0mr_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_text, train_label)):\n",
    "    print('\\n\\n------------fold:{}------------\\n'.format(fold))\n",
    "\n",
    "    text = train_text[train_index]\n",
    "    y = train_label[train_index]\n",
    "\n",
    "    val_text = train_text[valid_index]\n",
    "    val_y = train_label[valid_index]\n",
    "\n",
    "    train_D = data_generator([text, y], config, shuffle=True)\n",
    "    val_D = data_generator([val_text, val_y], config)\n",
    "\n",
    "    model = MODEL_CLASSES[config.model](config).to(config.device)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "    if config.pgd:\n",
    "        pgd = PGD(model)\n",
    "        K = 3\n",
    "    elif config.fgm:\n",
    "        fgm = FGM(model)\n",
    "\n",
    "    if config.focalloss:\n",
    "        loss_fn = FocalLoss(config.num_class)\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss()  # BCEWithLogitsLoss就是把Sigmoid-BCELoss合成一步\n",
    "\n",
    "\n",
    "    num_train_steps = int(len(train) / config.batch_size * config.epoch)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "    if config.Stratification:\n",
    "        bert_params = [x for x in param_optimizer if 'bert' in x[0]]\n",
    "        normal_params = [p for n, p in param_optimizer if 'bert' not in n]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in bert_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in bert_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "            {'params': normal_params, 'lr': config.normal_lr},\n",
    "        ]\n",
    "    else:\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_parameters, lr=config.learn_rate) # lr为全局学习率\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(len(train) / config.batch_size / 2),\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    PATH = './models/bert_{}.pth'.format(fold)\n",
    "    save_model_path = './models/'\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "\n",
    "    for e in range(config.epoch):\n",
    "        print('\\n------------epoch:{}------------'.format(e))\n",
    "        model.train()\n",
    "        acc = 0\n",
    "        train_len = 0\n",
    "        loss_num = 0\n",
    "        tq = tqdm(train_D,ncols=70,disable=True)\n",
    "        last=time.time()\n",
    "        for input_ids, input_masks, segment_ids, labels in tq:\n",
    "            label_t = torch.tensor(labels, dtype=torch.long).to(config.device)\n",
    "\n",
    "            y_pred = model(input_ids, input_masks, segment_ids)\n",
    "\n",
    "            loss = loss_fn(y_pred, label_t)\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "\n",
    "            if config.pgd:\n",
    "                pgd.backup_grad()\n",
    "                # 对抗训练\n",
    "                for t in range(K):\n",
    "                    pgd.attack(is_first_attack=(t == 0))  # 在embedding上添加对抗扰动, first attack时备份param.data\n",
    "                    if t != K - 1:\n",
    "                        model.zero_grad()\n",
    "                    else:\n",
    "                        pgd.restore_grad()\n",
    "                    y_pred = model(input_ids, input_masks, segment_ids)\n",
    "\n",
    "                    loss_adv = loss_fn(y_pred, label_t)\n",
    "                    loss_adv = loss_adv.mean()\n",
    "                    loss_adv.backward()  # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "                pgd.restore()  # 恢复embedding参数\n",
    "\n",
    "            elif config.fgm:\n",
    "                # 对抗训练\n",
    "                fgm.attack()  # 在embedding上添加对抗扰动\n",
    "                y_pred = model(input_ids, input_masks, segment_ids)\n",
    "                loss_adv = loss_fn(y_pred, label_t)\n",
    "                loss_adv = loss_adv.mean()\n",
    "                loss_adv.backward()  # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "                fgm.restore()  # 恢复embedding参数\n",
    "\n",
    "\n",
    "            # 梯度下降，更新参数\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "\n",
    "            y_pred = np.argmax(y_pred.detach().to(\"cpu\").numpy(), axis=1)\n",
    "            acc += sum(y_pred == labels)\n",
    "            loss_num += loss.item()\n",
    "            train_len += len(labels)\n",
    "            tq.set_postfix(fold=fold, epoch=e, loss=loss_num / train_len, acc=acc / train_len)\n",
    "        print(f\"微调第{e}轮耗时：{time.time()-last}\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_p = []\n",
    "            y_l = []\n",
    "            train_logit = None\n",
    "            for input_ids, input_masks, segment_ids, labels in tqdm(val_D,disable=True):\n",
    "                label_t = torch.tensor(labels, dtype=torch.long).to(config.device)\n",
    "\n",
    "                y_pred = model(input_ids, input_masks, segment_ids)\n",
    "                y_pred = F.softmax(y_pred)\n",
    "                y_pred = y_pred.detach().to(\"cpu\").numpy()\n",
    "                if train_logit is None:\n",
    "                    train_logit = y_pred\n",
    "                else:\n",
    "                    train_logit = np.vstack((train_logit, y_pred))\n",
    "\n",
    "                y_p += list(y_pred[:,1])\n",
    "\n",
    "                y_pred = np.argmax(y_pred, axis=1)\n",
    "                y_l += list(y_pred)\n",
    "\n",
    "\n",
    "            f1 = macro_f1(val_y, y_l)\n",
    "            print(\"best_f1:{} \\n\".format(f1))\n",
    "            if f1 >= best_f1:\n",
    "                best_f1 = f1\n",
    "                oof_train[valid_index] = np.array(train_logit)\n",
    "                #torch.save(model.module.state_dict() if hasattr(model, \"module\") else model.state_dict(), PATH)\n",
    "                torch.save(model.module if hasattr(model, \"module\") else model, PATH)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    del text\n",
    "    del y\n",
    "    del val_text\n",
    "    del val_y\n",
    "    del train_D\n",
    "    del val_D\n",
    "    del tq\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0d3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
