{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ebd883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 23:36:57.748508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "random.seed(0)\n",
    "np.random.seed(0)#seed应该在main里尽早设置，以防万一\n",
    "os.environ['PYTHONHASHSEED'] =str(0)#消除hash算法的随机性\n",
    "import transformers as _\n",
    "from transformers1 import BertTokenizer\n",
    "from Config import TOKENIZERS\n",
    "from tqdm import tqdm\n",
    "\n",
    "from NEZHA.configuration_nezha import NeZhaConfig\n",
    "from NEZHA.modeling_nezha import NeZhaForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd519c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "maxlen=64\n",
    "batch_size=64\n",
    "vocab_file_dir = './nezha_model/vocab.txt'\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_file_dir)\n",
    "\n",
    "def paddingList(ls:list,val,returnTensor=False):\n",
    "    ls=ls[:]#不要改变了原list尺寸\n",
    "    maxLen=max([len(i) for i in ls])\n",
    "    for i in range(len(ls)):\n",
    "        ls[i]=ls[i]+[val]*(maxLen-len(ls[i]))\n",
    "    return torch.tensor(ls,device='cuda') if returnTensor else ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cff9304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = NeZhaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512,\n",
    ")\n",
    "\n",
    "model = NeZhaForMaskedLM.from_pretrained(\"./nezha_model\").to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5cd0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/projects/log-based-failuer-diagnosis/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TOKENIZERS['NEZHA'].from_pretrained(vocab_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120d2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = pd.read_csv('../../data/preliminary_train_label_dataset.csv')\n",
    "label2 = pd.read_csv('../../data/preliminary_train_label_dataset_s.csv')\n",
    "label_df = pd.concat([label1, label2]).reset_index(drop=True)\n",
    "label_df = label_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3438c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv('../../data/preliminary_submit_dataset_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140b96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_csv('../../new_src/new_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcd2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['time'] = pd.to_datetime(log_df['time'])\n",
    "label_df['fault_time'] = pd.to_datetime(label_df['fault_time'])\n",
    "submit_df['fault_time'] = pd.to_datetime(submit_df['fault_time'])\n",
    "\n",
    "log_df['time_ts'] = log_df[\"time\"].values.astype(np.int64) // 10 ** 9\n",
    "label_df['fault_time_ts'] = label_df[\"fault_time\"].values.astype(np.int64) // 10 ** 9\n",
    "submit_df['fault_time_ts'] = submit_df[\"fault_time\"].values.astype(np.int64) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcda6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bert_feature(text):\n",
    "#     input_ids, input_masks, segment_ids = [], [], []\n",
    "#     tkRes = tokenizer(text, max_length=64, truncation='longest_first',\n",
    "#                            return_attention_mask=False)\n",
    "#     input_id = tkRes['input_ids']\n",
    "#     segment_id = tkRes['token_type_ids']\n",
    "#     input_ids.append(input_id)\n",
    "#     segment_ids.append(segment_id)\n",
    "\n",
    "#     input_ids = paddingList(input_ids, 0, returnTensor=True)\n",
    "#     segment_ids = paddingList(segment_ids, 0, returnTensor=True)\n",
    "#     input_masks = (input_ids != 0)\n",
    "\n",
    "#     feature = model(input_ids, input_masks, segment_ids)[0].cpu().detach().numpy()\n",
    "#     return np.mean(feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2fe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_feature(df):\n",
    "    features = []\n",
    "    for i in range(df.shape[0]):\n",
    "        input_ids, input_masks, segment_ids = [], [], []\n",
    "        text = df.iloc[i]['msg'].strip()\n",
    "        tkRes = tokenizer(text, max_length=64, truncation='longest_first',\n",
    "                               return_attention_mask=False)\n",
    "        input_id = tkRes['input_ids']\n",
    "        segment_id = tkRes['token_type_ids']\n",
    "        input_ids.append(input_id)\n",
    "        segment_ids.append(segment_id)\n",
    "\n",
    "        input_ids = paddingList(input_ids, 0, returnTensor=True)\n",
    "        segment_ids = paddingList(segment_ids, 0, returnTensor=True)\n",
    "        input_masks = (input_ids != 0)\n",
    "\n",
    "        features.append(model(input_ids, input_masks, segment_ids)[0].cpu().detach().numpy().mean(axis=1))\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(features.shape[0], features.shape[2])\n",
    "    features = np.mean(features, axis=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "021dbeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:10,  3.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18978/145399440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtr_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_bert_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtr_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m465\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18978/3761860849.py\u001b[0m in \u001b[0;36mget_bert_feature\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0minput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_data = []\n",
    "for idx, row in tqdm(label_df.iterrows()):\n",
    "    sn = row['sn']\n",
    "    fault_time = row['fault_time']\n",
    "    ts = row['fault_time_ts']\n",
    "    label = row['label']\n",
    "\n",
    "    df = log_df[log_df['sn'] == sn].copy()\n",
    "    df = df[df['time_ts'] <= ts].copy()\n",
    "    df = df.sort_values(by='time_ts').reset_index(drop=True)\n",
    "    df = df.tail(20).copy()\n",
    "\n",
    "    if df.shape[0] > 0:\n",
    "        tr_data.append(get_bert_feature(df))\n",
    "    else:\n",
    "        tr_data.append(np.zeros(465,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7768a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = np.array(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d36025",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data = []\n",
    "for idx, row in tqdm(submit_df.iterrows()):\n",
    "    sn = row['sn']\n",
    "    fault_time = row['fault_time']\n",
    "    ts = row['fault_time_ts']\n",
    "\n",
    "    df = log_df[log_df['sn'] == sn].copy()\n",
    "    df = df[df['time_ts'] <= ts].copy()\n",
    "    df = df.sort_values(by='time_ts').reset_index(drop=True)\n",
    "    df = df.tail(20).copy()\n",
    "\n",
    "    if df.shape[0] > 0:\n",
    "        te_data.append(get_bert_feature(df))\n",
    "    else:\n",
    "        te_data.append(np.zeros(461,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = np.array(te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = []\n",
    "sn_data = []\n",
    "fault_time_data = []\n",
    "for idx, row in tqdm(label_df.iterrows()):\n",
    "    label = row['label']\n",
    "    sn = row['sn']\n",
    "    fault_time = row['fault_time']\n",
    "    label_data.append(label)\n",
    "    sn_data.append(sn)\n",
    "    fault_time_data.append(fault_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_feature)\n",
    "train_df.columns = ['bert_%d'%i for i in range(465)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sn'] = sn_data\n",
    "train_df['fault_time'] = fault_time_data\n",
    "train_df['label'] = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_data = []\n",
    "fault_time_data = []\n",
    "for idx, row in tqdm(submit_df.iterrows()):\n",
    "    sn = row['sn']\n",
    "    fault_time = row['fault_time']\n",
    "    sn_data.append(sn)\n",
    "    fault_time_data.append(fault_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_feature)\n",
    "test_df.columns = ['bert_%d'%i for i in range(465)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['sn'] = sn_data\n",
    "test_df['fault_time'] = fault_time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f01191",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train5.csv', index=False)\n",
    "test_df.to_csv('test5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b8b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3372424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b16ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param target_df: [sn,fault_time,label]\n",
    "    :param submit_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    weights =  [3  /  7,  2  /  7,  1  /  7,  1  /  7]\n",
    "    overall_df = pd.DataFrame([y_true, y_pred]).T\n",
    "    overall_df.columns = ['label_gt', 'label_pr']\n",
    "\n",
    "    macro_F1 =  0.\n",
    "    for i in  range(len(weights)):\n",
    "        TP =  len(overall_df[(overall_df['label_gt'] == i) & (overall_df['label_pr'] == i)])\n",
    "        FP =  len(overall_df[(overall_df['label_gt'] != i) & (overall_df['label_pr'] == i)])\n",
    "        FN =  len(overall_df[(overall_df['label_gt'] == i) & (overall_df['label_pr'] != i)])\n",
    "        precision = TP /  (TP + FP)  if  (TP + FP)  >  0  else  0\n",
    "        recall = TP /  (TP + FN)  if  (TP + FP)  >  0  else  0\n",
    "        F1 =  2  * precision * recall /  (precision + recall)  if  (precision + recall)  >  0  else  0\n",
    "        macro_F1 += weights[i]  * F1\n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac130699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a48399c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.array(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28989aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/projects/log-based-failuer-diagnosis/env/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117503\n",
      "[LightGBM] [Info] Number of data points in the train set: 13283, number of used features: 461\n",
      "[LightGBM] [Info] Start training from score -2.420971\n",
      "[LightGBM] [Info] Start training from score -1.589536\n",
      "[LightGBM] [Info] Start training from score -0.577466\n",
      "[LightGBM] [Info] Start training from score -1.925345\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's multi_logloss: 0.824577\tvalid_1's multi_logloss: 0.850534\n",
      "[20]\ttraining's multi_logloss: 0.658913\tvalid_1's multi_logloss: 0.70363\n",
      "[30]\ttraining's multi_logloss: 0.558378\tvalid_1's multi_logloss: 0.617978\n",
      "[40]\ttraining's multi_logloss: 0.492121\tvalid_1's multi_logloss: 0.566407\n",
      "[50]\ttraining's multi_logloss: 0.445785\tvalid_1's multi_logloss: 0.533293\n",
      "[60]\ttraining's multi_logloss: 0.411038\tvalid_1's multi_logloss: 0.510574\n",
      "[70]\ttraining's multi_logloss: 0.384668\tvalid_1's multi_logloss: 0.495853\n",
      "[80]\ttraining's multi_logloss: 0.363562\tvalid_1's multi_logloss: 0.486547\n",
      "[90]\ttraining's multi_logloss: 0.346757\tvalid_1's multi_logloss: 0.480421\n",
      "[100]\ttraining's multi_logloss: 0.332207\tvalid_1's multi_logloss: 0.476625\n",
      "[110]\ttraining's multi_logloss: 0.319518\tvalid_1's multi_logloss: 0.473618\n",
      "[120]\ttraining's multi_logloss: 0.308315\tvalid_1's multi_logloss: 0.4719\n",
      "[130]\ttraining's multi_logloss: 0.298072\tvalid_1's multi_logloss: 0.470987\n",
      "[140]\ttraining's multi_logloss: 0.289028\tvalid_1's multi_logloss: 0.469985\n",
      "[150]\ttraining's multi_logloss: 0.280891\tvalid_1's multi_logloss: 0.46969\n",
      "[160]\ttraining's multi_logloss: 0.273606\tvalid_1's multi_logloss: 0.469683\n",
      "[170]\ttraining's multi_logloss: 0.266774\tvalid_1's multi_logloss: 0.470011\n",
      "Early stopping, best iteration is:\n",
      "[158]\ttraining's multi_logloss: 0.275046\tvalid_1's multi_logloss: 0.469521\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117502\n",
      "[LightGBM] [Info] Number of data points in the train set: 13283, number of used features: 461\n",
      "[LightGBM] [Info] Start training from score -2.420123\n",
      "[LightGBM] [Info] Start training from score -1.589905\n",
      "[LightGBM] [Info] Start training from score -0.577466\n",
      "[LightGBM] [Info] Start training from score -1.925345\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's multi_logloss: 0.826989\tvalid_1's multi_logloss: 0.845251\n",
      "[20]\ttraining's multi_logloss: 0.663058\tvalid_1's multi_logloss: 0.69512\n",
      "[30]\ttraining's multi_logloss: 0.563748\tvalid_1's multi_logloss: 0.607418\n",
      "[40]\ttraining's multi_logloss: 0.497755\tvalid_1's multi_logloss: 0.553717\n",
      "[50]\ttraining's multi_logloss: 0.451574\tvalid_1's multi_logloss: 0.518384\n",
      "[60]\ttraining's multi_logloss: 0.417715\tvalid_1's multi_logloss: 0.495354\n",
      "[70]\ttraining's multi_logloss: 0.391671\tvalid_1's multi_logloss: 0.479008\n",
      "[80]\ttraining's multi_logloss: 0.371061\tvalid_1's multi_logloss: 0.467779\n",
      "[90]\ttraining's multi_logloss: 0.353754\tvalid_1's multi_logloss: 0.459505\n",
      "[100]\ttraining's multi_logloss: 0.338988\tvalid_1's multi_logloss: 0.453328\n",
      "[110]\ttraining's multi_logloss: 0.325953\tvalid_1's multi_logloss: 0.448551\n",
      "[120]\ttraining's multi_logloss: 0.314578\tvalid_1's multi_logloss: 0.445322\n",
      "[130]\ttraining's multi_logloss: 0.304683\tvalid_1's multi_logloss: 0.442775\n",
      "[140]\ttraining's multi_logloss: 0.295496\tvalid_1's multi_logloss: 0.440467\n",
      "[150]\ttraining's multi_logloss: 0.287357\tvalid_1's multi_logloss: 0.439103\n",
      "[160]\ttraining's multi_logloss: 0.279603\tvalid_1's multi_logloss: 0.438461\n",
      "[170]\ttraining's multi_logloss: 0.2728\tvalid_1's multi_logloss: 0.437876\n",
      "[180]\ttraining's multi_logloss: 0.266487\tvalid_1's multi_logloss: 0.437331\n",
      "[190]\ttraining's multi_logloss: 0.260529\tvalid_1's multi_logloss: 0.437182\n",
      "[200]\ttraining's multi_logloss: 0.254723\tvalid_1's multi_logloss: 0.437084\n",
      "[210]\ttraining's multi_logloss: 0.24939\tvalid_1's multi_logloss: 0.437486\n",
      "Early stopping, best iteration is:\n",
      "[192]\ttraining's multi_logloss: 0.259358\tvalid_1's multi_logloss: 0.437052\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117511\n",
      "[LightGBM] [Info] Number of data points in the train set: 13283, number of used features: 461\n",
      "[LightGBM] [Info] Start training from score -2.420123\n",
      "[LightGBM] [Info] Start training from score -1.589905\n",
      "[LightGBM] [Info] Start training from score -0.577466\n",
      "[LightGBM] [Info] Start training from score -1.925345\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's multi_logloss: 0.823589\tvalid_1's multi_logloss: 0.849969\n",
      "[20]\ttraining's multi_logloss: 0.658736\tvalid_1's multi_logloss: 0.703257\n",
      "[30]\ttraining's multi_logloss: 0.559106\tvalid_1's multi_logloss: 0.619003\n",
      "[40]\ttraining's multi_logloss: 0.49371\tvalid_1's multi_logloss: 0.567477\n",
      "[50]\ttraining's multi_logloss: 0.44728\tvalid_1's multi_logloss: 0.534164\n",
      "[60]\ttraining's multi_logloss: 0.413198\tvalid_1's multi_logloss: 0.511243\n",
      "[70]\ttraining's multi_logloss: 0.386683\tvalid_1's multi_logloss: 0.495851\n",
      "[80]\ttraining's multi_logloss: 0.365534\tvalid_1's multi_logloss: 0.485328\n",
      "[90]\ttraining's multi_logloss: 0.348105\tvalid_1's multi_logloss: 0.478813\n",
      "[100]\ttraining's multi_logloss: 0.333335\tvalid_1's multi_logloss: 0.475486\n",
      "[110]\ttraining's multi_logloss: 0.320969\tvalid_1's multi_logloss: 0.472978\n",
      "[120]\ttraining's multi_logloss: 0.309561\tvalid_1's multi_logloss: 0.470467\n",
      "[130]\ttraining's multi_logloss: 0.299935\tvalid_1's multi_logloss: 0.468975\n",
      "[140]\ttraining's multi_logloss: 0.291213\tvalid_1's multi_logloss: 0.468157\n",
      "[150]\ttraining's multi_logloss: 0.282927\tvalid_1's multi_logloss: 0.46711\n",
      "[160]\ttraining's multi_logloss: 0.275434\tvalid_1's multi_logloss: 0.466717\n",
      "[170]\ttraining's multi_logloss: 0.268828\tvalid_1's multi_logloss: 0.466971\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's multi_logloss: 0.279085\tvalid_1's multi_logloss: 0.466705\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117506\n",
      "[LightGBM] [Info] Number of data points in the train set: 13283, number of used features: 461\n",
      "[LightGBM] [Info] Start training from score -2.420123\n",
      "[LightGBM] [Info] Start training from score -1.589536\n",
      "[LightGBM] [Info] Start training from score -0.577466\n",
      "[LightGBM] [Info] Start training from score -1.925861\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's multi_logloss: 0.826304\tvalid_1's multi_logloss: 0.84771\n",
      "[20]\ttraining's multi_logloss: 0.661806\tvalid_1's multi_logloss: 0.699105\n",
      "[30]\ttraining's multi_logloss: 0.561971\tvalid_1's multi_logloss: 0.612167\n",
      "[40]\ttraining's multi_logloss: 0.495937\tvalid_1's multi_logloss: 0.558217\n",
      "[50]\ttraining's multi_logloss: 0.450091\tvalid_1's multi_logloss: 0.524988\n",
      "[60]\ttraining's multi_logloss: 0.416114\tvalid_1's multi_logloss: 0.502733\n",
      "[70]\ttraining's multi_logloss: 0.389322\tvalid_1's multi_logloss: 0.487824\n",
      "[80]\ttraining's multi_logloss: 0.368175\tvalid_1's multi_logloss: 0.477977\n",
      "[90]\ttraining's multi_logloss: 0.35118\tvalid_1's multi_logloss: 0.471005\n",
      "[100]\ttraining's multi_logloss: 0.336639\tvalid_1's multi_logloss: 0.465806\n",
      "[110]\ttraining's multi_logloss: 0.324045\tvalid_1's multi_logloss: 0.462334\n",
      "[120]\ttraining's multi_logloss: 0.312858\tvalid_1's multi_logloss: 0.460117\n",
      "[130]\ttraining's multi_logloss: 0.303011\tvalid_1's multi_logloss: 0.45897\n",
      "[140]\ttraining's multi_logloss: 0.293994\tvalid_1's multi_logloss: 0.458193\n",
      "[150]\ttraining's multi_logloss: 0.286088\tvalid_1's multi_logloss: 0.458128\n",
      "[160]\ttraining's multi_logloss: 0.278669\tvalid_1's multi_logloss: 0.458207\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's multi_logloss: 0.291554\tvalid_1's multi_logloss: 0.457841\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117505\n",
      "[LightGBM] [Info] Number of data points in the train set: 13284, number of used features: 461\n",
      "[LightGBM] [Info] Start training from score -2.420199\n",
      "[LightGBM] [Info] Start training from score -1.589612\n",
      "[LightGBM] [Info] Start training from score -0.577541\n",
      "[LightGBM] [Info] Start training from score -1.925420\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttraining's multi_logloss: 0.826315\tvalid_1's multi_logloss: 0.845512\n",
      "[20]\ttraining's multi_logloss: 0.661358\tvalid_1's multi_logloss: 0.695768\n",
      "[30]\ttraining's multi_logloss: 0.561927\tvalid_1's multi_logloss: 0.609819\n",
      "[40]\ttraining's multi_logloss: 0.496067\tvalid_1's multi_logloss: 0.555868\n",
      "[50]\ttraining's multi_logloss: 0.450284\tvalid_1's multi_logloss: 0.52148\n",
      "[60]\ttraining's multi_logloss: 0.416412\tvalid_1's multi_logloss: 0.498558\n",
      "[70]\ttraining's multi_logloss: 0.39055\tvalid_1's multi_logloss: 0.483038\n",
      "[80]\ttraining's multi_logloss: 0.369342\tvalid_1's multi_logloss: 0.472462\n",
      "[90]\ttraining's multi_logloss: 0.35188\tvalid_1's multi_logloss: 0.464482\n",
      "[100]\ttraining's multi_logloss: 0.337195\tvalid_1's multi_logloss: 0.45957\n",
      "[110]\ttraining's multi_logloss: 0.324496\tvalid_1's multi_logloss: 0.456352\n",
      "[120]\ttraining's multi_logloss: 0.313371\tvalid_1's multi_logloss: 0.454789\n",
      "[130]\ttraining's multi_logloss: 0.303343\tvalid_1's multi_logloss: 0.453951\n",
      "[140]\ttraining's multi_logloss: 0.293998\tvalid_1's multi_logloss: 0.452574\n",
      "[150]\ttraining's multi_logloss: 0.285734\tvalid_1's multi_logloss: 0.451439\n",
      "[160]\ttraining's multi_logloss: 0.278449\tvalid_1's multi_logloss: 0.451096\n",
      "[170]\ttraining's multi_logloss: 0.271364\tvalid_1's multi_logloss: 0.45126\n",
      "[180]\ttraining's multi_logloss: 0.264947\tvalid_1's multi_logloss: 0.451307\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's multi_logloss: 0.274112\tvalid_1's multi_logloss: 0.451069\n",
      "0.5644196709203607\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros((test_feature.shape[0], 4))\n",
    "val_preds = np.zeros((train_feature.shape[0], 4))\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(train_feature, train_label):\n",
    "    xtrain, ytrain = train_feature[train_index], train_label[train_index]\n",
    "    xtest, ytest = train_feature[test_index], train_label[test_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(xtrain, label=ytrain)\n",
    "    dvalid = lgb.Dataset(xtest, label=ytest)\n",
    "    param = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'early_stopping_rounds': 20,\n",
    "        'learning_rate': 0.03,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    gbm = lgb.train(\n",
    "        param, dtrain, valid_sets=[dtrain, dvalid], num_boost_round=100000, verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    val_preds[test_index] = gbm.predict(xtest)\n",
    "    preds += gbm.predict(test_feature) / 5\n",
    "\n",
    "print(macro_f1(train_label, np.argmax(val_preds, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd25d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97332386",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df['label'] = np.argmax(preds, axis=1)\n",
    "submit_df[['sn', 'fault_time', 'label']].to_csv('./preliminary_pred_a.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8202f8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5786237446986044"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5786237446986044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5829e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
