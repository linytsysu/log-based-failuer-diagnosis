{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e376cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16604, 3) (3030, 2)\n",
      "(16604, 5) (3030, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=16, random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from tsfresh.feature_extraction.feature_calculators import abs_energy, benford_correlation, count_above, \\\n",
    "    count_above_mean, mean_abs_change, mean_change, percentage_of_reoccurring_datapoints_to_all_datapoints, \\\n",
    "    percentage_of_reoccurring_values_to_all_values, sample_entropy\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "stage = 'test'\n",
    "\n",
    "label1 = pd.read_csv('../data/preliminary_train_label_dataset.csv')\n",
    "label2 = pd.read_csv('../data/preliminary_train_label_dataset_s.csv')\n",
    "label_df = pd.concat([label1, label2]).reset_index(drop=True)\n",
    "label_df = label_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "if stage == 'final_a':\n",
    "    submit_df = pd.read_csv('/tcdata/final_submit_dataset_a.csv')\n",
    "else:\n",
    "    submit_df = pd.read_csv('../data/preliminary_submit_dataset_b.csv')\n",
    "\n",
    "print(label_df.shape, submit_df.shape)\n",
    "\n",
    "log_df = pd.read_csv('../user_data/log_template.csv')\n",
    "log_df['msg_lower'] = log_df['msg_lower'].astype(str)\n",
    "log_df['server_model'] = log_df['server_model'].astype(str)\n",
    "\n",
    "log_df['time'] = pd.to_datetime(log_df['time'])\n",
    "label_df['fault_time'] = pd.to_datetime(label_df['fault_time'])\n",
    "submit_df['fault_time'] = pd.to_datetime(submit_df['fault_time'])\n",
    "\n",
    "log_df['time_ts'] = log_df[\"time\"].values.astype(np.int64) // 10 ** 9\n",
    "label_df['fault_time_ts'] = label_df[\"fault_time\"].values.astype(np.int64) // 10 ** 9\n",
    "submit_df['fault_time_ts'] = submit_df[\"fault_time\"].values.astype(np.int64) // 10 ** 9\n",
    "\n",
    "if stage == 'final_a':\n",
    "    crashdump_df1 = pd.read_csv('../data/preliminary_crashdump_dataset.csv')\n",
    "    venus_df1 = pd.read_csv('../data/preliminary_venus_dataset.csv')\n",
    "    crashdump_df2 = pd.read_csv('/tcdata/final_crashdump_dataset_a.csv')\n",
    "    venus_df2 = pd.read_csv('/tcdata/final_venus_dataset_a.csv')\n",
    "    crashdump_df = pd.concat([crashdump_df1, crashdump_df2]).reset_index(drop=True)\n",
    "    venus_df = pd.concat([venus_df1, venus_df2]).reset_index(drop=True)\n",
    "else:\n",
    "    crashdump_df = pd.read_csv('../data/preliminary_crashdump_dataset.csv')\n",
    "    venus_df = pd.read_csv('../data/preliminary_venus_dataset.csv')\n",
    "crashdump_df['fault_time'] = pd.to_datetime(crashdump_df['fault_time'])\n",
    "venus_df['fault_time'] = pd.to_datetime(venus_df['fault_time'])\n",
    "crashdump_df['fault_time_ts'] = crashdump_df[\"fault_time\"].values.astype(np.int64) // 10 ** 9\n",
    "venus_df['fault_time_ts'] = venus_df[\"fault_time\"].values.astype(np.int64) // 10 ** 9\n",
    "\n",
    "label_df = label_df.merge(log_df[['sn', 'server_model']].drop_duplicates(), on=['sn'], how='left')\n",
    "submit_df = submit_df.merge(log_df[['sn', 'server_model']].drop_duplicates(), on=['sn'], how='left')\n",
    "label_df = label_df.fillna('MISSING')\n",
    "submit_df = submit_df.fillna('MISSING')\n",
    "print(label_df.shape, submit_df.shape)\n",
    "\n",
    "label_cnt_df = label_df.groupby('label').size().reset_index().rename({0: 'label_cnt'}, axis=1)\n",
    "label_model_cnt_df = label_df.groupby(['server_model', 'label']).size().reset_index()\\\n",
    "    .rename({0: 'label_model_cnt'}, axis=1)\n",
    "label_model_cnt_df = label_model_cnt_df.merge(label_cnt_df, on='label', how='left')\n",
    "label_model_cnt_df['model/label'] = label_model_cnt_df['label_model_cnt'] / label_model_cnt_df['label_cnt']\n",
    "\n",
    "# counter_map = {}\n",
    "# for idx in tqdm(range(label_df.shape[0])):\n",
    "#     row = label_df.iloc[idx]\n",
    "#     sn = row['sn']\n",
    "#     fault_time_ts = row['fault_time_ts']\n",
    "#     sub_log = log_df[(log_df['sn'] == sn) & (log_df['time_ts'] <= fault_time_ts)]\n",
    "#     sub_log = sub_log.sort_values(by='time')\n",
    "#     df_tmp = sub_log.tail(20)\n",
    "#     k = '%d'%(row['label'])\n",
    "#     if not k in counter_map:\n",
    "#         counter_map[k] = Counter()\n",
    "#     counter_map[k].update(np.unique(df_tmp['msg_id'].values.tolist()))\n",
    "# for k in counter_map:\n",
    "#     counter_map[k] = [item[0] for item in counter_map[k].most_common()[:50]]\n",
    "\n",
    "def safe_split(strs, n, sep='|'):\n",
    "    str_li = strs.split(sep)\n",
    "    if len(str_li) >= n + 1:\n",
    "        return str_li[n].strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "log_df['time_gap'] = log_df['time'].dt.ceil('30T')\n",
    "log_df['msg_split_0'] = log_df['msg_lower'].apply(lambda x: safe_split(x, 0))\n",
    "log_df['msg_split_1'] = log_df['msg_lower'].apply(lambda x: safe_split(x, 1))\n",
    "log_df['msg_split_2'] = log_df['msg_lower'].apply(lambda x: safe_split(x, 2))\n",
    "\n",
    "sentences_list = list()\n",
    "for info, group in log_df.groupby(['sn', 'time_gap']):\n",
    "    group = group.sort_values(by='time')\n",
    "    sentences_list.append(\"\\n\".join(group['msg_lower'].values.astype(str)))\n",
    "\n",
    "sentences = list()\n",
    "for s in sentences_list:\n",
    "    sentences.append([w for w in s.split()])\n",
    "\n",
    "w2v_model = Word2Vec(sentences, vector_size=64, window=3, min_count=2, sg=0, hs=1, workers=1, seed=2022)\n",
    "\n",
    "# tokenized_sent = [word_tokenize(s.lower()) for s in sentences_list]\n",
    "# tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "# d2v_model = Doc2Vec(tagged_data, vector_size=32, window=3, min_count=2, workers=1, seed=2022)\n",
    "# d2v_model.random.seed(2022)\n",
    "\n",
    "tfv = TfidfVectorizer(ngram_range=(1,3), min_df=5, max_features=50000)\n",
    "tfv.fit(sentences_list)\n",
    "X_tfidf = tfv.transform(sentences_list)\n",
    "svd = TruncatedSVD(n_components=16, random_state=42)\n",
    "svd.fit(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bba371c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16604, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58d042a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 16604/16604 [10:12<00:00, 27.10it/s]\n"
     ]
    }
   ],
   "source": [
    "sub_log_cnt = []\n",
    "for idx in tqdm(range(label_df.shape[0])):\n",
    "    row = label_df.iloc[idx]\n",
    "    sn = row['sn']\n",
    "    fault_time = row['fault_time']\n",
    "    fault_time_ts = row['fault_time_ts']\n",
    "    server_model = row['server_model']\n",
    "    sub_log = log_df[(log_df['sn'] == sn) & (log_df['time_ts'] <= fault_time_ts)]\n",
    "    sub_log = sub_log.sort_values(by='time')\n",
    "    \n",
    "    df_tmp1 = sub_log.tail(10)\n",
    "    sub_log_cnt.append(df_tmp1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f787ee3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3030/3030 [01:42<00:00, 29.61it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sub_log_cnt = []\n",
    "for idx in tqdm(range(submit_df.shape[0])):\n",
    "    row = submit_df.iloc[idx]\n",
    "    sn = row['sn']\n",
    "    fault_time = row['fault_time']\n",
    "    fault_time_ts = row['fault_time_ts']\n",
    "    server_model = row['server_model']\n",
    "    sub_log = log_df[(log_df['sn'] == sn) & (log_df['time_ts'] <= fault_time_ts)]\n",
    "    sub_log = sub_log.sort_values(by='time')\n",
    "\n",
    "    df_tmp1 = sub_log.tail(10)\n",
    "    test_sub_log_cnt.append(df_tmp1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddba7ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.603228137798121, 3.436963696369637)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sub_log_cnt), np.mean(test_sub_log_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73456f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
